use mixed 8bit quant
WARNING 11-01 18:45:12 config.py:1718] The model's config.json does not contain any of the following keys to determine the original maximum length of the model: ['max_position_embeddings', 'n_positions', 'max_seq_len', 'seq_length', 'model_max_length', 'max_sequence_length', 'max_seq_length', 'seq_len']. Assuming the model's maximum length is 2048.
WARNING 11-01 18:45:12 config.py:322] mixq8bit quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 11-01 18:45:12 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='/home/chenyidong/data/mixqdata/quant8/falcon-7b', speculative_config=None, tokenizer='/home/chenyidong/data/mixqdata/quant8/falcon-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=mixq8bit, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/chenyidong/data/mixqdata/quant8/falcon-7b, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 11-01 18:45:12 selector.py:258] Cannot use FlashAttention-2 backend because the vllm.vllm_flash_attn package is not found. Make sure that vllm_flash_attn was built and installed (on by default).
INFO 11-01 18:45:12 selector.py:116] Using XFormers backend.
INFO 11-01 18:45:13 model_runner.py:1014] Starting to load model /home/chenyidong/data/mixqdata/quant8/falcon-7b...
--get_quant_method---

--get_quant_method---

INFO 11-01 18:45:13 selector.py:258] Cannot use FlashAttention-2 backend because the vllm.vllm_flash_attn package is not found. Make sure that vllm_flash_attn was built and installed (on by default).
INFO 11-01 18:45:13 selector.py:116] Using XFormers backend.
--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
--get_quant_method---

--get_quant_method---

--get_quant_method---

--get_quant_method---
down
INFO 11-01 18:45:21 model_runner.py:1025] Loading model weights took 13.1738 GB
INFO 11-01 18:45:21 gpu_executor.py:122] # GPU blocks: 60303, # CPU blocks: 32768
INFO 11-01 18:45:23 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-01 18:45:23 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-01 18:45:29 model_runner.py:1456] Graph capturing finished in 5 secs.
Prompt: 'Hello, my name is', Generated text: ''
Prompt: 'The president of the United States is', Generated text: ''
Prompt: 'The capital of France is', Generated text: ''
Prompt: 'The future of AI is', Generated text: ''
